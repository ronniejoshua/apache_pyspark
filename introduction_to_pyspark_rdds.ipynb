{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Resilient Distributed Datasets or RDDs were the primary API for version one and they're still \n",
    "available in Spark version two. Now almost all the code we've been running using DataFrames \n",
    "compiles down to an RDD. So it makes sense for us to have a basic understanding of what an RDD is. \n",
    "\n",
    "An RDD is an immutable partitioned collection of records that can be worked on in parallel. \n",
    "\n",
    "\n",
    "Now remember that with a DataFrame, each record is a structured row containing fields with a known \n",
    "schema. In the case of RDD, the records are just Java, Scala or Python objects. And so you have complete \n",
    "control over them. \n",
    "\n",
    "\n",
    "Although this has several advantages, there are a couple of challenges. Spark does not understand the \n",
    "inner structure of your records as it does with your DataFrames. This means that the optimizations you \n",
    "would have automatically got with DataFrames, you will need to manually recreate. \n",
    "\n",
    "\n",
    "The RDD APIs are available in Python as well as Scala and Java. You can get good performance with running \n",
    "RDDs with Scala and Java. However, running Python RDDs, is like running Python user-defined functions row by row. \n",
    "\n",
    "\n",
    "So we need to serialize the data to the Python process, work on it in Python and then serialize it back \n",
    "to the Java Virtual Machine. For this reason, it's recommended to stick with the high level APIs in Spark \n",
    "and only use the RDDs when absolutely necessary. \n",
    "\n",
    "\n",
    "If you're curious about the performance difference between RDDs and DataFrames, take a look at \n",
    "this blog post. \n",
    "\n",
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "When the databricks team, first introduced DataFrames back in 2015. The team ran a couple of experiments \n",
    "and compared the performance of Scala and Python grouped by aggregation on 10 million integer pairs on a \n",
    "single machine. In the case of DataFrames, both Scala and Python operations are compiled into JVM byte code. \n",
    "So there's little difference between them. Python and Scala DataFrames had two times better performance than \n",
    "Scala RDDs. But wait for it. The same DataFrames, had a whopping five times better performance than Python RDDs. \n",
    "\n",
    "\n",
    "So when would you use RDDs? \n",
    "\n",
    "    This is when you need fine grained control over the physical distribution and partition of data. \n",
    "\n",
    "    Another possibility is that you are having to maintain some legacy codebase written using RDDs\n",
    "    in Spark version one. \n",
    "\n",
    "Now, RDDs are a low level API that are powerful but lack a lot of the optimizations that we have seen \n",
    "with DataFrames. With RDDs, you also do not have access to the built in functions that you do when using \n",
    "DataFrames. This means that you must define each filter, map and aggregation as a function. \n",
    "\n",
    "Transformations, actions and lazy evaluations work in the same way as they do with DataFrames. If you're \n",
    "ever uncertain whether a given function is a transformation or an action in RDD land, look at its return \n",
    "type. Transformations return RDDs while as actions return some other data type. \n",
    "\n",
    "Let's take a look at some transformations. \n",
    "\n",
    "    With map, you defined a function and then apply it record by record. \n",
    "    \n",
    "    Flatmap returns a new RDD by first applying a function to all of the elements RDDs and \n",
    "    then flattening the result.\n",
    "    \n",
    "    Filter, returns a new RDD. Meaning only the elements that satisfy a condition. \n",
    "\n",
    "    With reduce, we are taking neighboring elements and producing a single combined result. \n",
    "    For example, let's say you have a set of numbers. You can reduce this to its sum by providing a function \n",
    "    that takes as input two values and reduces them to one. \n",
    "    \n",
    "    Count works in the same way that we've seen in DataFrames and allows you to count the number of rows \n",
    "    in an RDD.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.12:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps_rdd = sc.textFile('./data./police-station.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_header = ps_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rest = ps_rdd.filter(lambda line: line != ps_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rest.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rest.map(lambda line: line.split(',')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_rest.map(lambda line: line.split(',')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ps_rest.filter(lambda line: line.split(',')[0] == '7').\n",
    "     map(lambda line: (line.split(',')[0],\n",
    "                       line.split(',')[1],\n",
    "                       line.split(',')[2],\n",
    "                       line.split(',')[5]\n",
    "                      )).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ps_rest.filter(lambda line: line.split(',')[0] in [10, 11])\n",
    "        .map(lambda line: (line.split(',')[1],\n",
    "                          line.split(',')[2]\n",
    "                          line.split(',')[5]\n",
    "                          )).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
