{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "file_path = f'{data_path}/reported-crimes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp,col,lit\n",
    "\n",
    "rc = (spark.read\n",
    "            .csv(file_path,header=True)\n",
    "            .withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a'))\n",
    "            .filter(col('Date') <= lit('2018-11-11')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_schema = st.StructType([\n",
    "    st.StructField('ID', st.StringType(), True),\n",
    "    st.StructField('Case Number', st.StringType(), True),\n",
    "    st.StructField('Date', st.TimestampType(), True),\n",
    "    st.StructField('Block', st.StringType(), True),\n",
    "    st.StructField('IUCR', st.StringType(), True),\n",
    "    st.StructField('Primary Type', st.StringType(), True),\n",
    "    st.StructField('Description', st.StringType(), True),\n",
    "    st.StructField('Location Description', st.StringType(), True),\n",
    "    st.StructField('Domestic', st.BooleanType(), True),\n",
    "    st.StructField('Beat', st.StringType(), True),\n",
    "    st.StructField('Ward', st.StringType(), True),\n",
    "    st.StructField('Community Area', st.StringType(), True),\n",
    "    st.StructField('FBI Code', st.StringType(), True),\n",
    "    st.StructField('X Coordinate', st.StringType(), True),\n",
    "    st.StructField('Y Coordinate', st.StringType(), True),\n",
    "    st.StructField('Year', st.IntegerType(), True),\n",
    "    st.StructField('Updated On', st.TimestampType(), True),\n",
    "    st.StructField('Latitude', st.BooleanType(), True),\n",
    "    st.StructField('Longitude', st.BooleanType(), True),\n",
    "    st.StructField('Location', st.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcs = (spark.read\n",
    "            .csv(file_path,header=True, schema=rc_schema)\n",
    "            .withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a'))\n",
    "            .filter(col('Date') <= lit('2018-11-11')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.take(n), df.collect(), df.show(), df.limit(), df.head()\n",
    "# df.columnName, df['columnName'], df.select(col('columnName'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select('columnName1', 'columnName2')\n",
    "# df[['columnName1', 'columnName2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('IUCR').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(rc.IUCR).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rc.select(col('IUCR')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('Case Number', 'Date', 'Arrest').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.withColumn('One', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rc = rc.drop('IUCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aday_inc = (spark.read\n",
    "            .csv(file_path,header=True)\n",
    "            .withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a'))\n",
    "            .filter(col('Date') == lit('2018-11-12')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aday_inc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.union(aday_inc).orderBy('Date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rc.groupBy('Primary type')\n",
    "    .count()\n",
    "    .orderBy('count', ascending=False)\n",
    "    .show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select('Arrest').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.filter(col('Arrest') == 'true').count() / rc.select(col('Arrest')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rc.groupBy('Location Description')\n",
    "    .count()\n",
    "    .orderBy('count', ascending=False)\n",
    "    .show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(\n",
    "    fnc.lower(col('Primary Type')), \n",
    "    fnc.upper(col('Primary Type')), \n",
    "    fnc.substring('Primary Type', 1, 4)\n",
    "    ).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(\n",
    "    fnc.min(col('Date')), \n",
    "    fnc.max(col('Date'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(\n",
    "    fnc.date_sub(\n",
    "        fnc.min(col('Date')), 3),\n",
    "    fnc.date_add(\n",
    "        fnc.max(col('Date')), 3)\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2019-12-25 13:30:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "                            [('2019-12-25 13:30:00',)],\n",
    "                            ['Christmas']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(fnc.to_date(col('Christmas'), 'yyyy-MM-dd HH:mm:ss'),\n",
    "          fnc.to_timestamp(col('Christmas'), 'yyyy-MM-dd HH:mm:ss')\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "                            [('25/Dec/2019 13:30:00',)],\n",
    "                            ['Christmas']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(fnc.to_date(col('Christmas'), 'dd/MMM/yyyy HH:mm:ss'),\n",
    "          fnc.to_timestamp(col('Christmas'), 'dd/MMM/yyyy HH:mm:ss')\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "                            [('12/25/2019 01:30:00 PM',)],\n",
    "                            ['Christmas']\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(fnc.to_date(col('Christmas'), 'MM/dd/yyyy hh:mm:ss a'),\n",
    "          fnc.to_timestamp(col('Christmas'), 'MM/dd/yyyy hh:mm:ss a')\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "file_path = f'{data_path}/police-station.csv'\n",
    "ps = spark.read.csv(file_path,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.select(fn.col('DISTRICT')).distinct().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "file_path = f'{data_path}/reported-crimes.csv'\n",
    "rc = (spark.read\n",
    "            .csv(file_path,header=True)\n",
    "            .withColumn('Date',fn.to_timestamp(fn.col('Date'),'MM/dd/yyyy hh:mm:ss a'))\n",
    "            .filter(fn.col('Date') >= fn.lit('2018-11-11')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.cache()\n",
    "rc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(fn.col('District')).distinct().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.select(fn.lpad(fn.col('DISTRICT'), 3, '0')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = ps.withColumn('format_district', fn.lpad(fn.col('DISTRICT'), 3, '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_col_drop = (\n",
    "             'ADDRESS',\n",
    "             'CITY',\n",
    "             'STATE',\n",
    "             'ZIP',\n",
    "             'WEBSITE',\n",
    "             'PHONE',\n",
    "             'FAX',\n",
    "             'TTY',\n",
    "             'X COORDINATE',\n",
    "             'Y COORDINATE',\n",
    "             'LATITUDE',\n",
    "             'LONGITUDE',\n",
    "             'LOCATION',\n",
    "             'format_district')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rc.join(ps, rc.District == ps.format_district, 'left_outer').drop(*ps_col_drop).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(fn.col('Primary Type')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(fn.col('Primary Type')).distinct().orderBy(fn.col('Primary Type')).show(35, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = rc.filter(\n",
    "    (fn.col('Primary Type') == 'NON-CRIMINAL') | \n",
    "    (fn.col('Primary Type') == 'NON - CRIMINAL') | \n",
    "    (fn.col('Primary Type') == 'NON - CRIMINAL (SUBJECT SPECIFIED)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.select(fn.col('Primary Type')).distinct().orderBy(fn.col('Primary Type')).show(35, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.groupBy(fn.col('Description')).count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.select(\n",
    "    fn.col('Date'), \n",
    "    fn.dayofweek(fn.col('Date')),\n",
    "    fn.date_format(fn.col('Date'), 'E')\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.groupBy(fn.date_format(fn.col('Date'), 'E')).count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.groupBy(fn.date_format(fn.col('Date'), 'E')).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow = [x[0] for x in rc.groupBy(fn.date_format(fn.col('Date'), 'E')).count().collect()]\n",
    "cnt = [x[1] for x in rc.groupBy(fn.date_format(fn.col('Date'), 'E')).count().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
