{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkContext\n",
    "\"\"\"\n",
    "SparkContext is the entry gate of Apache Spark functionality. The most important step \n",
    "of any Spark driver application is to generate SparkContext. It allows your Spark \n",
    "Application to access Spark Cluster with the help of Resource Manager (YARN/Mesos).\n",
    "\"\"\"\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "\"\"\"\n",
    "SparkSession is the entry point to Spark SQL. It is one of the very first objects you \n",
    "create while developing a Spark SQL application. As a Spark developer, you create a \n",
    "SparkSession using the SparkSession.builder method (that gives you access to Builder \n",
    "API that you use to configure the session).\n",
    "\"\"\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                  .appName(\"Spark SQL Query Dataframes\")\n",
    "                  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name', StringType(), False),\n",
    "  StructField('age', IntegerType(), False),\n",
    "  StructField('city', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.format('csv').load(name='rawdata.csv', schema=peopleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Using lazy processing\n",
    "\n",
    "Lazy processing operations will usually return in about the same amount of time regardless \n",
    "of the actual quantity of data. Remember that this is due to Spark not performing any \n",
    "transformations until an action is requested.\n",
    "\n",
    "When working with Spark that no transformations take effect until you apply an action. This can \n",
    "be confusing at times, but is one of the underpinnings of Spark's power.\n",
    "\"\"\"\n",
    "\n",
    "data_path = './data'\n",
    "file_path = f'{data_path}/AA_DFW_2014_Departures_Short.csv.gz'\n",
    "\n",
    "# Load the CSV file\n",
    "aa_dfw_df = (spark.read\n",
    "                    .format('csv')\n",
    "                    .options(Header=True)\n",
    "                    .load(file_path))\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show(3)\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', fn.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Difculties with CSV files:\n",
    "--------------------------\n",
    "No defined schema\n",
    "Nested data requires special handling\n",
    "Encoding format limited\n",
    "\n",
    "Spark and CSV files\n",
    "-------------------\n",
    "Slow to parse \n",
    "Files cannot be altered(no\"predicate pushdown\")\n",
    "Any intermediate use requires redefining schema\n",
    "\n",
    "The Parquet Format\n",
    "-------------------\n",
    "A columnar data format\n",
    "Supported in Spark and other data processing frameworks\n",
    "Supports predicate pushdown\n",
    "Automatically stores schema information\n",
    "\n",
    "\n",
    "When working with Spark, you'll often start with CSV, JSON, or other data sources. This provides \n",
    "a lot of flexibility for the types of data to load, but it is not an optimal format for Spark. \n",
    "The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means \n",
    "Spark will only process the data necessary to complete the operations you define versus reading \n",
    "the entire dataset. This gives Spark more flexibility in accessing the data and often drastically \n",
    "improves performance on large datasets.\n",
    "\n",
    "\n",
    "Reading Parquet Files\n",
    "---------------------\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "\n",
    "df = spark.read.parquet('filename.parquet')\n",
    "\n",
    "\n",
    "Writing Parquet Files\n",
    "---------------------\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "\n",
    "df.write.parquet('filename.parquet')\n",
    "\n",
    "\n",
    "Parquet as backing stores for SparkSQL operations\n",
    "-------------------------------------------------\n",
    "\n",
    "flight_df = spark.read.parquet('flights.parquet')\n",
    "\n",
    "flight_df.createOrReplaceTempView('flights')\n",
    "\n",
    "short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataFrame refresher\n",
    "\n",
    "DataFrames:\n",
    "    Made up of rows & columns\n",
    "    Immutable\n",
    "    Use various transformation operations to modify data\n",
    "\n",
    "\n",
    "\n",
    "# Return rows where name starts with \"M\"\n",
    "----------------------------------------\n",
    "voter_df.filter(voter_df.name.like('M%'))\n",
    "\n",
    "\n",
    "\n",
    "# Return name and position only \n",
    "-------------------------------\n",
    "voters = voter_df.select(fn.col('name'), fn.col('position'))\n",
    "\n",
    "\n",
    "# Filter/Where\n",
    "--------------\n",
    "voter_df.filter(voter_df.date > '1/1/2019')\n",
    "\n",
    "voter_df.where(...)\n",
    "\n",
    "\n",
    "# Select\n",
    "--------\n",
    "voter_df.select(voter_df.name)\n",
    "\n",
    "\n",
    "# withColumn\n",
    "------------\n",
    "voter_df.withColumn('year', voter_df.date.year)\n",
    "\n",
    "\n",
    "# drop\n",
    "------\n",
    "voter_df.drop('unused_column')\n",
    "\n",
    "\n",
    "\n",
    "Filtering data\n",
    "---------------\n",
    "    Remove nulls\n",
    "    Remove odd entries\n",
    "    Split data from combined sources\n",
    "    Negate with ~\n",
    "\n",
    "\n",
    "# Illustrations\n",
    "---------------\n",
    "voter_df.filter(voter_df['name'].isNotNull())\n",
    "\n",
    "voter_df.filter(voter_df.date.year > 1800)\n",
    "\n",
    "voter_df.where(voter_df['_c0'].contains('VOTE'))\n",
    "\n",
    "voter_df.where(~ voter_df._c1.isNull())\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Applied per column as transformation\n",
    "--------------------------------------\n",
    "voter_df.withColumn('upper', fn.upper('name'))\n",
    "\n",
    "# Can create intermediary columns\n",
    "---------------------------------\n",
    "voter_df.withColumn('splits', fn.split('name', ' '))\n",
    "\n",
    "# Can cast to other types\n",
    "-------------------------\n",
    "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))\n",
    "\n",
    "# Various utility functions / transformations to interact with ArrayType()\n",
    "------------------------------------------------------------------------\n",
    "    .size(<column>) - returns length of arrayType() column\n",
    "    \n",
    "    .getItem(<index>) - used to retrieve a specic item at index of list column.\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Conditional Clauses are: Inline version of if / then / else\n",
    "-------------------------------------------------------------\n",
    ".when()\n",
    "\n",
    ".otherwise()\n",
    "\n",
    ".when(<if condition>, <then x>)\n",
    "\n",
    "# Example 1\n",
    "-----------\n",
    "df.select(df.Name, df.Age, fn.when(df.Age >= 18, \"Adult\")\n",
    "\n",
    "\n",
    "# Example 2\n",
    "-----------\n",
    "df.select(df.Name, df.Age,           \n",
    "            .when(df.Age >= 18, \"Adult\")          \n",
    "            .when(df.Age < 18, \"Minor\"))\n",
    "            \n",
    "            \n",
    "# Example 3: .otherwise() is like else\n",
    "------------------------------------\n",
    "df.select(df.Name, df.Age,          \n",
    "            .when(df.Age >= 18, \"Adult\")          \n",
    "            .otherwise(\"Minor\"))          \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# User defined functions or UDFs\n",
    "---------------------------------\n",
    "    Python method\n",
    "    Wrapped via the pyspark.sql.functions.udf method\n",
    "    Stored as a variable\n",
    "    Called like a normal Spark function\n",
    "\n",
    "\n",
    "# Define a Python method\n",
    "-------------------------\n",
    "def reverseString(mystr):\n",
    "    return mystr[::-1]\n",
    "    \n",
    "\n",
    "# Wrap the function and store as a variable\n",
    "--------------------------------------------\n",
    "udfReverseString = udf(reverseString, StringType())\n",
    "\n",
    "\n",
    "# Use with Spark\n",
    "-----------------\n",
    "user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\n",
    "\n",
    "\n",
    "# Argument-less example\n",
    "-----------------------\n",
    "def sortingCap():\n",
    "    return random.choice(['G', 'H', 'R', 'S'])\n",
    "\n",
    "udfSortingCap = udf(sortingCap, StringType())\n",
    "\n",
    "user_df = user_df.withColumn('Class', udfSortingCap())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Partitioning \n",
    "------------\n",
    "DataFrames are brokenup into partitions\n",
    "Partition size can vary \n",
    "Each partition is handled independently\n",
    "\n",
    "\n",
    "\n",
    "Lazy processing\n",
    "---------------\n",
    "\n",
    "Transformations are lazy\n",
    "    .withColumn(...)\n",
    "    \n",
    "    .select(...)\n",
    "\n",
    "Nothing is actually done until an action is performed\n",
    "    .count()\n",
    "    \n",
    "    .write(...)\n",
    "\n",
    "\n",
    "Transformations can be re-ordered for best performance\n",
    "\n",
    "Sometimes causes unexpected behavior\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select(voter_df['VOTER_NAME']).distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ fn.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', fn.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(fn.size('splits') - 1))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == 'Councilmember', fn.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                                   when(voter_df.TITLE == 'Councilmember', fn.rand())\n",
    "                                   .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                                   .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = fn.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', fn.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of partitions in each DataFrame\n",
    "print(f'There are {voter_df.rdd.getNumPartitions()} partitions in the voter_df DataFrame')\n",
    "print(f'There are {voter_df_single.rdd.getNumPartitions()} partitions in the voter_df_single DataFrame.')\n",
    "\n",
    "# Add a ROW_ID field to each DataFrame\n",
    "voter_df = voter_df.withColumn('ROW_ID', fn.monotonically_increasing_id())\n",
    "voter_df_single = voter_df_single.withColumn('ROW_ID', fn.monotonically_increasing_id())\n",
    "\n",
    "# Show the top 10 IDs in each DataFrame \n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
    "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the highest ROW_ID and save it in previous_max_ID\n",
    "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
    "\n",
    "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
    "voter_df_april = voter_df_april.withColumn('ROW_ID', fn.monotonically_increasing_id() + previous_max_ID)\n",
    "\n",
    "# Show the ROW_ID from both DataFrames and compare\n",
    "voter_df_march.select('ROW_ID').show()\n",
    "voter_df_april.select('ROW_ID').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full and split files into DataFrames\n",
    "full_df = spark.read.csv('departures_full.txt.gz')\n",
    "split_df = spark.read.csv('departures_0*.txt.gz')\n",
    "\n",
    "# Print the count and run time for each DataFrame\n",
    "start_time_a = time.time()\n",
    "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
    "\n",
    "start_time_b = time.time()\n",
    "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
    "print(\"Time to run: %f\" % (time.time() - start_time_b))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Awesome! The results should illustrate that using split files runs more quickly than using one \n",
    "large file for import. Note that in certain circumstances the results may be reversed. This is \n",
    "a side effect of running as a single node cluster. Depending on the tasks required and resources \n",
    "available, it may occasionally take longer than expected. If you perform multiple runs of the \n",
    "tasks, you should see the full file import as generally slower than the split file import.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading Spark configurations\n",
    "\n",
    "You've recently configured a cluster via a cloud provider. Your only access is via the command \n",
    "shell or your python code. You'd like to verify some Spark settings to validate the configuration \n",
    "of the cluster.\n",
    "\n",
    "\n",
    "Using the spark.conf object allows you to validate the settings of a cluster without having \n",
    "configured it initially. This can help you know what changes should be optimized for your needs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Name of the Spark application instance\n",
    "app_name = spark.conf.get('spark.app.name')\n",
    "\n",
    "\n",
    "# Driver TCP port\n",
    "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
    "\n",
    "\n",
    "# Number of join partitions\n",
    "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
    "\n",
    "\n",
    "# Show the results\n",
    "print(\"Name: %s\" % app_name)\n",
    "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
    "print(\"Number of partitions: %s\" % num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the number of partitions in variable\n",
    "before = departures_df.rdd.getNumPartitions()\n",
    "\n",
    "# Configure Spark to use 500 partitions\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
    "\n",
    "# Recreate the DataFrame using the departures data file\n",
    "departures_df = spark.read.csv('./data/departures.txt.gz').distinct()\n",
    "\n",
    "# Print the number of partitions for each instance\n",
    "print(\"Partition count before change: %d\" % before)\n",
    "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "It's important to remember that modifying the settings in Spark may change objects that already exist. \n",
    "Sometimes the changes only take effect after configuring a new DataFrame. Remember to test changes \n",
    "you make to Spark configurations to verify it does exactly what you think.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the flights_df and aiports_df DataFrames\n",
    "normal_df = flights_df.join(airports_df, \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan\n",
    "normal_df.explain()\n",
    "\n",
    "\"\"\"\n",
    "You've implemented a basic join and examined the query plan. Learning to parse a query plan \n",
    "will help you understand what Spark is doing and when\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using broadcasting on Spark joins:\n",
    "\n",
    "Remember that table joins in Spark are split between the cluster workers. If the data is not local, \n",
    "various shuffle operations are required and can have a negative impact on performance. Instead, we're \n",
    "going to use Spark's broadcast operations to give each node a copy of the specified data.\n",
    "\n",
    "A couple tips:\n",
    "    Broadcast the smaller DataFrame. The larger the DataFrame, the more time required to transfer to \n",
    "    the worker nodes.\n",
    "    \n",
    "    On small DataFrames, it may be better skip broadcasting and let Spark figure out any optimization \n",
    "    on its own.\n",
    "    \n",
    "    If you look at the query execution plan, a broadcastHashJoin indicates you've successfully configured \n",
    "    broadcasting.\n",
    "\"\"\"\n",
    "\n",
    "# Import the broadcast method from pyspark.sql.functions\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join the flights_df and airports_df DataFrames using broadcasting\n",
    "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
    "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
    "\n",
    "# Show the query plan and compare against the original\n",
    "broadcast_df.explain()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Using Spark broadcasting to improve the performance of your data operations. You should see that the query \n",
    "plan uses the Broadcast operations instead of the default Spark versions. You'll likely use broadcasting \n",
    "often with production datasets - checking the query plan will help validate your configuration without \n",
    "actually running the tasks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your manager would like to see a simple pipeline example including the basic steps. For this example, \n",
    "you'll want to ingest a data file, filter a few rows, add an ID column to it, then write it out as JSON data.\n",
    "\"\"\"\n",
    "\n",
    "# Import the data to a DataFrame\n",
    "departures_df = spark.read.csv('./data/2015-departures.csv.gz', header=True)\n",
    "\n",
    "# Remove any duration of 0\n",
    "departures_df = departures_df.filter(departures_df[3] > 0)\n",
    "\n",
    "# Add an ID column\n",
    "departures_df = departures_df.withColumn('id', fn.monotonically_increasing_id())\n",
    "\n",
    "# Write the file out to JSON format\n",
    "departures_df.write.json('output.json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the file to a DataFrame and perform a row count\n",
    "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
    "full_count = annotations_df.count()\n",
    "\n",
    "# Count the number of rows beginning with '#'\n",
    "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
    "\n",
    "# Import the file to a new DataFrame, without commented rows\n",
    "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
    "\n",
    "# Count the new DataFrame and verify the difference is as expected\n",
    "no_comments_count = no_comments_df.count()\n",
    "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))\n",
    "\n",
    "\n",
    "\n",
    "# Split _c0 on the tab character and store the list in a variable\n",
    "tmp_fields = fn.split(annotations_df['_c0'], '\\t')\n",
    "\n",
    "# Create the colcount column on the DataFrame\n",
    "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
    "\n",
    "# Remove any rows containing fewer than 5 fields\n",
    "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\n",
    "\n",
    "# Count the number of rows\n",
    "final_count = annotations_df_filtered.count()\n",
    "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))\n",
    "\n",
    "\n",
    "\n",
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = F.split(annotations_df[\"_c0\"], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the content of _c0 on the tab character (aka, '\\t')\n",
    "split_cols = fn.split(annotations_df[\"_c0\"], '\\t')\n",
    "\n",
    "# Add the columns folder, filename, width, and height\n",
    "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
    "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
    "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
    "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
    "\n",
    "# Add split_cols as a column\n",
    "split_df = split_df.withColumn('split_cols', split_cols)\n",
    "\n",
    "\n",
    "# Rename the column in valid_folders_df\n",
    "valid_folders_df = valid_folders_df.withColumnRenamed('_c0', 'folder')\n",
    "\n",
    "# Count the number of rows in split_df\n",
    "split_count = split_df.count()\n",
    "\n",
    "# Join the DataFrames\n",
    "joined_df = split_df.join(fn.broadcast(valid_folders_df), \"folder\")\n",
    "\n",
    "# Compare the number of rows remaining\n",
    "joined_count = joined_df.count()\n",
    "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))\n",
    "\n",
    "\n",
    "\n",
    "# Determine the row counts for each DataFrame\n",
    "split_count = split_df.count()\n",
    "joined_count = joined_df.count()\n",
    "\n",
    "# Create a DataFrame containing the invalid rows\n",
    "invalid_df = split_df.join(fn.broadcast(joined_df), 'folder', 'left_anti')\n",
    "\n",
    "# Validate the count of the new DataFrame is as expected\n",
    "invalid_count = invalid_df.count()\n",
    "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
    "\n",
    "# Determine the number of distinct folder rows removed\n",
    "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
    "print(\"%d distinct invalid folders found\" % invalid_folder_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dog details and show 10 untruncated rows\n",
    "print(joined_df.select('dog_list').show(10, truncate=False))\n",
    "\n",
    "# Define a schema type for the details in the dog list\n",
    "DogType = StructType([\n",
    "    StructField(\"breed\", StringType(), False),\n",
    "    StructField(\"start_x\", IntegerType(), False),\n",
    "    StructField(\"start_y\", IntegerType(), False),\n",
    "    StructField(\"end_x\", IntegerType(), False),\n",
    "    StructField(\"end_y\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "\n",
    "# Create a function to return the number and type of dogs as a tuple\n",
    "def dogParse(doglist):\n",
    "    dogs = []\n",
    "    for dog in doglist:\n",
    "        (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
    "        dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
    "    return dogs\n",
    "\n",
    "# Create a UDF\n",
    "udfDogParse = fn.udf(dogParse, ArrayType(DogType))\n",
    "\n",
    "# Use the UDF to list of dogs and drop the old column\n",
    "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
    "\n",
    "# Show the number of dogs in the first 10 rows\n",
    "joined_df.select(fn.size('dogs')).show(10)\n",
    "\n",
    "\n",
    "\n",
    "# Define a UDF to determine the number of pixels per image\n",
    "def dogPixelCount(doglist):\n",
    "    totalpixels = 0\n",
    "    for dog in doglist:\n",
    "        totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
    "    return totalpixels\n",
    "\n",
    "# Define a UDF for the pixel count\n",
    "udfDogPixelCount = fn.udf(dogPixelCount, IntegerType())\n",
    "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
    "\n",
    "# Create a column representing the percentage of pixels\n",
    "joined_df = joined_df.withColumn(\n",
    "                                    'dog_percent', \n",
    "                                    (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100\n",
    "                                    )\n",
    "\n",
    "# Show the first 10 annotations with more than 60% dog\n",
    "joined_df.where('dog_percent > 60').show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
