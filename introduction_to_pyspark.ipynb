{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reference videos:\n",
    "    https://www.youtube.com/watch?v=5dARTeE6OpU\n",
    "    https://www.youtube.com/watch?v=e5ol7oyKV0A\n",
    "\n",
    "\n",
    "reference blogs:\n",
    "    https://www.edureka.co/blog/pyspark-rdd/\n",
    "    \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "Clustered computing: Collection of resources of multiple machines\n",
    "\n",
    "Parallel computing: Simultaneous computation\n",
    "\n",
    "Distributed computing: Collection of nodes (networked computers) that run in parallel\n",
    "\n",
    "Batch processing: Breaking the job into small pieces and running them on individual machines\n",
    "\n",
    "Real-time processing: Immediate processing of data\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features of Apache Spark framework\n",
    "\n",
    "Distributed cluster computing framework\n",
    "\n",
    "Efcient in-memory computations for large data sets\n",
    "\n",
    "Lightning fast data processing framework\n",
    "\n",
    "Provides support for Java, Scala, Python, R and SQL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apache Spark => Google Dataproc / Amazon Elastic MapReduce\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark modes of deployment\n",
    "\n",
    "    Local mode: Single machine such as your laptop\n",
    "        Local model convenient for testing, debugging and demonstration\n",
    "        \n",
    "    Cluster mode: Set of pre-dened machines\n",
    "        Good for production\n",
    "        \n",
    "    Workow: Local -> clusters\n",
    "    \n",
    "    No code change necessary\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over \n",
    "clusters with multiple nodes (think of each node as a separate computer). Splitting up your \n",
    "data makes it easier to work with very large datasets because each node only works with a \n",
    "small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total \n",
    "calculations required, so that both data processing and computation are performed in parallel \n",
    "over the nodes in the cluster. It is a fact that parallel computation can make certain types of \n",
    "programming tasks much faster.\n",
    "\n",
    "However, with greater computing power comes greater complexity.\n",
    "\n",
    "Deciding whether or not Spark is the best solution for your problem takes some experience, but you \n",
    "can consider questions like:\n",
    "\n",
    "    Is my data too big to work with on a single machine?\n",
    "    \n",
    "    Can my calculations be easily parallelized?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using Spark in Python\n",
    "\n",
    "The first step in using Spark is connecting to a cluster.\n",
    "\n",
    "In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. \n",
    "There will be one computer, called the master that manages splitting up the data and the computations. \n",
    "\n",
    "The master is connected to the rest of the computers in the cluster, which are called worker. The master \n",
    "sends the workers data and calculations to run, and they send their results back to the master.\n",
    "\n",
    "When you're just getting started with Spark it's simpler to just run a cluster locally. \n",
    "\n",
    "Creating the connection is as simple as creating an instance of the SparkContext class. The class constructor \n",
    "takes a few optional arguments that allow you to specify the attributes of the cluster you're connecting to.\n",
    "\n",
    "An object holding all these attributes can be created with the SparkConf() constructor. Take a look at \n",
    "the documentation for all the details!\n",
    "\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SparkContext is the entry gate of Apache Spark functionality. The most important step \n",
    "of any Spark driver application is to generate SparkContext. It allows your Spark \n",
    "Application to access Spark Cluster with the help of Resource Manager (YARN/Mesos).\n",
    "\"\"\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# Verify SparkContext\n",
    "print(sc)\n",
    "\n",
    "\n",
    "# Print Spark version\n",
    "print(sc.version)\n",
    "\n",
    "print(sc.pythonVer)\n",
    "\n",
    "print(sc.master)\n",
    "\n",
    "\"\"\"\n",
    "You'll probably notice that code takes longer to run than you might expect. This is because Spark is some \n",
    "serious software. It takes more time to start up than you might be used to. You may also find that running \n",
    "simpler computations might take longer than expected. That's because all the optimizations that Spark has \n",
    "under its hood are designed for complicated operations with big data sets. That means that for simple or \n",
    "small problems Spark may actually perform worse than some other solutions!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using DataFrames\n",
    "\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object \n",
    "that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs \n",
    "are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built \n",
    "on top of RDDs.\n",
    "\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns \n",
    "and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized \n",
    "for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the \n",
    "same result, but some often take much longer than others. When using RDDs, it's up to the data scientist \n",
    "to figure out the right way to optimize the query, but the DataFrame implementation has much of this \n",
    "optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. \n",
    "You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface \n",
    "with that connection.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "\"\"\"\n",
    "SparkSession is the entry point to Spark SQL. It is one of the very first objects you \n",
    "create while developing a Spark SQL application. As a Spark developer, you create a \n",
    "SparkSession using the SparkSession.builder method (that gives you access to Builder \n",
    "API that you use to configure the session).\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                  .appName(\"Apache PySpak Intro\")\n",
    "                  .getOrCreate())\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your SparkSession has an attribute called catalog which lists all the data inside the cluster. \n",
    "This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "One of the most useful is the .listTables() method, which returns the names of all the tables in \n",
    "your cluster as a list.\n",
    "\n",
    "Catalog is new API in spark 2.0 which allows us to interact with metadata of spark sql. This is a much \n",
    "better interface to metadata compared to earlier versions of spark.\n",
    "\"\"\"\n",
    "\n",
    "# Print the tables in the catalog\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sometimes it makes sense to then take that table and work with it locally using a tool like pandas. \n",
    "Spark DataFrames make that easy with the .toPandas() method. Calling this method on a Spark DataFrame \n",
    "returns the corresponding pandas DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "pandas_df = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Put some Spark in your data\n",
    "\n",
    "Put a pandas DataFrame into a Spark cluster! The SparkSession class has a method for this as well.\n",
    "The .createDataFrame() method takes a pandas DataFrame and returns a Spark DataFrame.\n",
    "\n",
    "The output of this method is stored locally, not in the SparkSession catalog. This means that you can \n",
    "use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the .sql() method) that references your DataFrame will throw an error. \n",
    "To access the data in this way, you have to save it as a temporary table.\n",
    "\n",
    "You can do this using the .createTempView() Spark DataFrame method, which takes as its only argument \n",
    "the name of the temporary table you'd like to register. This method registers the DataFrame as a table in \n",
    "the catalog, but as this table is temporary, it can only be accessed from the specific SparkSession used to \n",
    "create the Spark DataFrame.\n",
    "\n",
    "There is also the method .createOrReplaceTempView(). This safely creates a new temporary table if nothing \n",
    "was there before, or updates an existing table if one was already defined. You'll use this method to avoid \n",
    "running into problems with duplicate tables.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\".data/data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10), minPartitions = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\".data/data.txt\", minPartitions = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions in rdd\n",
    "\n",
    "print(\"Number of partitions in rdd is\", rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformations create new RDDS\n",
    "Actions perform computation on the RDDs\n",
    "\"\"\"\n",
    "\n",
    "# Map Transformation\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "rdd_map = rdd.map(lambda x: x * x)\n",
    "\n",
    "\n",
    "# Filter Transformation\n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "rdd_filter = rdd.filter(lambda x: x > 2)\n",
    "\n",
    "# Flatmap Transformation\n",
    "rdd = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "rdd_flatamp = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "\n",
    "\n",
    "# Union Transformation\n",
    "input_rdd = sc.textFile(\"logs.txt\")\n",
    "error_rdd = input_rdd.filter(lambda x: \"error\" in x.split())\n",
    "warnings_rdd = input_rdd.filter(lambda x: \"warnings\" in x.split())\n",
    "combined_rdd = error_rdd.union(warnings_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Actions: Operation return a value after running a computation on the RDD\n",
    "\n",
    "# collect() return all the elements of the dataset as an array\n",
    "rdd_map.collect()\n",
    "\n",
    "\n",
    "# take(N) returns an array with the first N elements of the dataset\n",
    "rdd_map.take(2)\n",
    "\n",
    "# first() prints the first element of the RDD\n",
    "rdd_map.first()\n",
    "\n",
    "\n",
    "# count() return the number of elements in the RDD\n",
    "rdd_map.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Introduction to pair RDDs in PySpark\n",
    "\n",
    "    Real life datasets are usually key/value pairs\n",
    "    Each row is a key and maps to one or more values\n",
    "    PairRDD is a special data structure to work with this kind of datasets\n",
    "    PairRDD: Key is the identier and value is data\n",
    "    \n",
    "Creating pair RDDs\n",
    "\n",
    "    Two common ways to create pairRDDs\n",
    "        From a list of key-value tuple\n",
    "        From a regularRDD\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into key/value form for paired RDD\n",
    "\n",
    "# Method 1\n",
    "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\n",
    "\n",
    "pairRDD_tuple = sc.parallelize(my_tuple)\n",
    "\n",
    "\n",
    "\n",
    "# Method 2\n",
    "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\n",
    "\n",
    "regularRDD = sc.parallelize(my_list)\n",
    "\n",
    "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey() transformation\n",
    "\n",
    "    # reduceByKey() transformation combines values with the same key\n",
    "    # It runs parallel operations for each key in the dataset\n",
    "    # It is a transformation and not action\n",
    "\n",
    "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34),\n",
    "                                (\"Neymar\", 22), (\"Messi\", 24)])\n",
    "\n",
    "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
    "\n",
    "pairRDD_reducebykey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortByKey() transformation\n",
    "\n",
    "    # sortByKey() operation orders pairRDD by key\n",
    "    # It returns an RDD sorted by key in ascending or descending order\n",
    "\n",
    "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey() transformation\n",
    "\n",
    "    # groupByKey() groups allthe values with the same key in the pairRDD\n",
    "    \n",
    "regularRDD = sc.parallelize(airports)\n",
    "\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join() transformation\n",
    "\n",
    "    # join() transformation joins the two pairRDDs based on their key\n",
    "    \n",
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
    "\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])    \n",
    "\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce() action\n",
    "\n",
    "    # reduce(func) action is used for aggregating the elements of a regularRDD\n",
    "    # The function should be commutative and associative\n",
    "\n",
    "x = [1,3,4,6]\n",
    "RDD = sc.parallelize(x)\n",
    "RDD.reduce(lambda x, y : x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveAsTextFile() action\n",
    "\n",
    "    # saveAsTextFile() action saves RDD into a text file inside a directory with \n",
    "    # each partition as a separate file\n",
    "\n",
    "RDD.saveAsTextFile(\"tempFile\")\n",
    "\n",
    "# coalesce() method can be used to save RDD as a single text file\n",
    "\n",
    "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Operations on pair RDDs\n",
    "\n",
    "    # RDD actions available for PySpark pairRDDs\n",
    "    # PairRDD actions leverage the key-value data\n",
    "\n",
    "\n",
    "# countByKey() action\n",
    "\n",
    "    # countByKey() only available for type (K,V)\n",
    "    # countByKey() action counts the number of elements for each key\n",
    "\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "for key, val in rdd.countByKey().items():\n",
    "    print(key, val)\n",
    "\n",
    "\n",
    "    \n",
    "# collectAsMap() action\n",
    "\n",
    "    # collectAsMap() return the key-value pairs in the RDD as a dictionary\n",
    "    \n",
    "# Example of collectAsMap() on a simple tuple:\n",
    "\n",
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD to DataFrame\n",
    "\n",
    "# Create a list of tuples\n",
    "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\n",
    "\n",
    "# Create a RDD from the list\n",
    "rdd = sc.parallelize(sample_list)\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\n",
    "\n",
    "# Check the type of names_df\n",
    "print(\"The type of names_df is\", type(names_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "file_path = f'{data_path}/location_temp.csv'\n",
    "\n",
    "sdf1 = (spark.read\n",
    "       .format(\"csv\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{data_path}/utilization.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV File does not have headers\n",
    "\n",
    "sdf2 = (spark.read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"false\")\n",
    "        .option(\"inferSchema\",\"true\")\n",
    "        .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2 = (sdf2.withColumnRenamed(\"_c0\", \"event_datetime\")\n",
    "            .withColumnRenamed(\"_c1\", \"server_id\")\n",
    "            .withColumnRenamed(\"_c2\", \"cpu_utilization\")\n",
    "            .withColumnRenamed(\"_c3\", \"free_memory\")\n",
    "            .withColumnRenamed(\"_c4\", \"session_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3_json_file_path = f'{data_path}/location_temp.json'\n",
    "sdf1.write.json(sdf3_json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4_json_file_path = f'{data_path}/utilization.json'\n",
    "sdf2.write.json(sdf4_json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3 = (spark.read\n",
    "            .format(\"json\")\n",
    "            .load(sdf3_json_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4 = (spark.read\n",
    "            .format(\"json\")\n",
    "            .load(sdf4_json_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls './data/utilization.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4_sample = sdf4.sample(withReplacement=False, fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf4_sort = sdf4_sample.sort('event_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.filter(sdf3[\"location_id\"]==\"loc0\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.filter(sdf3[\"location_id\"]==\"loc0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.filter(\"location_id = 'loc1'\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.groupBy(\"location_id\").count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.orderBy(\"location_id\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf3.groupby('location_id')\n",
    "    .agg({'temp_celcius': 'mean'})\n",
    "    .show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf3.groupby('location_id')\n",
    "    .agg({'temp_celcius': 'max'})\n",
    "    .show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sdf3.groupBy(\"location_id\")\n",
    "     .agg({'temp_celcius': 'mean'})\n",
    "     .orderBy(\"location_id\")\n",
    "     .show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.write.csv('./data/sdf3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls './data/sdf3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head './data/sdf3.csv/part-00000-f5051421-4bc9-4dc7-8e87-612f86d01554-c000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf3.write.json('./data/sdf3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls './data/sdf3.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head './data/sdf3.json/part-00000-be0377a9-4877-4524-92ce-8e611155b4c5-c000.json'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
