{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "                  .appName(\"Spark SQL Query Dataframes\")\n",
    "                  .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "file_path = f'{data_path}/utilization.json'\n",
    "sdf = (spark.read\n",
    "        .format(\"json\")\n",
    "        .load(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark SQL Table and giving it a name to be used in queries\n",
    "\n",
    "sdf.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT * FROM utilization LIMIT 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SHOW COLUMNS FROM utilization\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"DESCRIBE utilization\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT server_id, session_count FROM utilization LIMIT 3\")\n",
    "sdf_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT server_id as sid, session_count as sc FROM utilization\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT * FROM utilization WHERE server_id = 120\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT server_id, session_count FROM utilization WHERE session_count > 70\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT server_id, session_count FROM utilization WHERE session_count > 70 AND server_id = 120\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"\"\"\n",
    "                   SELECT server_id, session_count\n",
    "                   FROM utilization\n",
    "                   WHERE session_count > 70 AND server_id = 120\n",
    "                   ORDER BY session_count DESC\n",
    "                 \"\"\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT count(*) FROM utilization\")\n",
    "sdf_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"SELECT count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70\")\n",
    "sdf_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"\"\"SELECT server_id, count(*)\n",
    "                    FROM utilization\n",
    "                    WHERE session_count > 70\n",
    "                    GROUP BY server_id\"\"\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"\"\"\n",
    "                    SELECT server_id, count(*)\n",
    "                    FROM utilization\n",
    "                    WHERE session_count > 70\n",
    "                    GROUP BY server_id\n",
    "                    ORDER BY count(*) DESC\n",
    "                    \"\"\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"\"\"\n",
    "                    SELECT server_id, min(session_count), avg(session_count), max(session_count)\n",
    "                    FROM utilization\n",
    "                    WHERE session_count > 70\n",
    "                    GROUP BY server_id\n",
    "                    ORDER BY count(*) DESC\n",
    "                \"\"\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_sql = spark.sql(\"\"\"\n",
    "                    SELECT server_id, min(session_count), round(avg(session_count),2), max(session_count)\n",
    "                    FROM utilization\n",
    "                    WHERE session_count > 70\n",
    "                    GROUP BY server_id\n",
    "                    ORDER BY count(*) DESC\n",
    "                    \"\"\")\n",
    "sdf_sql.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{data_path}/utilization.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util = (spark.read\n",
    "               .format(\"json\")\n",
    "               .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_util.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'{data_path}/server_name.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server = (spark.read\n",
    "       .format(\"csv\")\n",
    "       .option(\"header\", \"true\")\n",
    "       .load(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server.createOrReplaceTempView(\"server_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = spark.sql(\"SELECT DISTINCT server_id FROM utilization ORDER BY server_id\")\n",
    "df_count.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT min(server_id), max(server_id) FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM server_name\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_join = spark.sql(\"\"\"\n",
    "                         SELECT u.server_id, sn.server_name, u.session_count\n",
    "                         FROM utilization AS u\n",
    "                         INNER JOIN server_name AS sn\n",
    "                         ON sn.server_id = u.server_id\n",
    "                    \"\"\")\n",
    "sdf_join.show(3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup = spark.sparkContext.parallelize([\n",
    "                             Row(server_name='101 Server', cpu_utilization=85, session_count=80),\n",
    "                             Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=80),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=80)\n",
    "                        ]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup.drop_duplicates(['server_name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.describe('cpu_utilization','free_memory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.stat.corr('cpu_utilization','free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.stat.freqItems(('server_id','session_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) FROM utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization)\n",
    "            FROM utilization\n",
    "            GROUP BY server_id\n",
    "          \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT server_id, FLOOR(cpu_utilization*100/10) AS bucket FROM utilization').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_window = \"\"\"\n",
    "                SELECT \n",
    "                event_datetime, \n",
    "                server_id, \n",
    "                cpu_utilization,\n",
    "                avg(cpu_utilization) OVER (PARTITION BY server_id) AS avg_server_util\n",
    "                FROM utilization\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(sql_window).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_window = \"\"\"\n",
    "                SELECT \n",
    "                event_datetime, \n",
    "                server_id, \n",
    "                cpu_utilization,\n",
    "                avg(cpu_utilization) OVER (PARTITION BY server_id) AS avg_server_util,\n",
    "                cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) AS delta_server_util\n",
    "                FROM utilization\n",
    "            \"\"\"\n",
    "spark.sql(sql_window).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_window = \"\"\"\n",
    "                SELECT \n",
    "                event_datetime, \n",
    "                server_id, \n",
    "                cpu_utilization,\n",
    "                avg(cpu_utilization) OVER(\n",
    "                                            PARTITION BY server_id \n",
    "                                            ORDER BY event_datetime\n",
    "                                            ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING\n",
    "                                        ) AS avg_server_util\n",
    "                FROM utilization\n",
    "            \"\"\"\n",
    "spark.sql(sql_window).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.sparkContext.parallelize(\n",
    "                            [Row(server_name='101 Server', cpu_utilization=85, session_count=80),\n",
    "                             Row(server_name='101 Server', cpu_utilization=80, session_count=90),\n",
    "                             Row(server_name='102 Server', cpu_utilization=85, session_count=40),\n",
    "                             Row(server_name='103 Server', cpu_utilization=70, session_count=80),\n",
    "                             Row(server_name='104 Server', cpu_utilization=60, session_count=80)]\n",
    "                            ).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.withColumn('na_col', lit(None).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na.fillna('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_na.fillna('A').union(df_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"na_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM na_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trainsched.txt\n",
    "df = spark.read.csv(\"./data/trainsched.txt\", header=True)\n",
    "\n",
    "# Create temporary table called schedule\n",
    "\"\"\"\n",
    "A dataframe can be used to create a temporary table. A temporary table is one that \n",
    "will not exist after the session ends. Spark documentation also refers to this type \n",
    "of table as a SQL temporary view. In the documentation this is referred to as to \n",
    "register the dataframe as a SQL temporary view. This command is called on the dataframe \n",
    "itself, and creates a table if it does not already exist, replacing it with the current \n",
    "data from the dataframe if it does already exist.\n",
    "\"\"\"\n",
    "\n",
    "df.createOrReplaceTempView(\"schedule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the columns in the table df\n",
    "spark.sql(\"DESCRIBE schedule\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "sql_window = \"\"\"\n",
    "                SELECT \n",
    "                    train_id, \n",
    "                    station, \n",
    "                    time,\n",
    "                    LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time ASC) AS time_next\n",
    "                FROM schedule\n",
    "            \"\"\"\n",
    "# Run the query and display the result\n",
    "spark.sql(sql_window).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "sql_window = \"\"\"\n",
    "                SELECT \n",
    "                    train_id, \n",
    "                    station, \n",
    "                    time,\n",
    "                    diff_min,\n",
    "                    SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time ASC) AS running_total\n",
    "                FROM schedule\n",
    "            \"\"\"\n",
    "# Run the query and display the result\n",
    "spark.sql(sql_window).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add col running_total that sums diff_min col in each group\n",
    "sql_window = \"\"\"\n",
    "                SELECT \n",
    "                    ROW_NUMBER() OVER (ORDER BY time ASC) AS row,\n",
    "                    train_id, \n",
    "                    station, \n",
    "                    time,\n",
    "                    LEAD(time,1) OVER (ORDER BY time ASC) AS time_next,\n",
    "                    diff_min,\n",
    "                    SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time ASC) AS running_total\n",
    "                FROM schedule\n",
    "            \"\"\"\n",
    "# Run the query and display the result\n",
    "spark.sql(sql_window).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the identical result in each command\n",
    "spark.sql(\"\"\"\n",
    "                SELECT \n",
    "                    train_id, MIN(time) AS start \n",
    "                    FROM schedule \n",
    "                    GROUP BY train_id\n",
    "            \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupBy('train_id')\n",
    "        .agg({'time':'min'})\n",
    "        .withColumnRenamed('min(time)', 'start')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the identical result in each command\n",
    "spark.sql(\"\"\"\n",
    "                SELECT \n",
    "                    train_id, MIN(time), MAX(time)\n",
    "                    FROM schedule \n",
    "                    GROUP BY train_id\n",
    "            \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupBy('train_id')\n",
    "        .agg({'time':'min', 'time':'max'})\n",
    "        .withColumnRenamed('min(time)', 'start')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a SQL query giving a result identical to dot_df\n",
    "spark.sql(\"\"\"\n",
    "                SELECT \n",
    "                    train_id, MIN(time) AS start, MAX(time) AS end \n",
    "                FROM schedule \n",
    "                GROUP BY train_id\n",
    "            \"\"\")\n",
    "\n",
    "sql_df = spark.sql(query)\n",
    "\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Obtain the identical result using dot notation \n",
    "dot_df = df.withColumn(\n",
    "                        'time_next', \n",
    "                        lead('time', 1).over(\n",
    "                                                Window.partitionBy('train_id').orderBy('time')\n",
    "                                            )\n",
    "                      )\n",
    "\n",
    "\n",
    "dot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert window function from dot notation to SQL\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "\n",
    "dot_df = df.withColumn(\n",
    "                        'diff_min', \n",
    "                        (\n",
    "                            unix_timestamp(\n",
    "                                        lead('time', 1).over(Window.partitionBy('train_id').orderBy('time')),\n",
    "                                        'H:m') -\n",
    "                            unix_timestamp('time', 'H:m')\n",
    "                        )/60\n",
    ")\n",
    "\n",
    "dot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a SQL query giving a result identical to dot_df\n",
    "spark.sql(\"\"\"\n",
    "                SELECT \n",
    "                    *, \n",
    "                    (\n",
    "                        UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \n",
    "                         - UNIX_TIMESTAMP(time, 'H:m')\n",
    "                    )/60 AS diff_min \n",
    "                FROM schedule \n",
    "            \"\"\")\n",
    "\n",
    "sql_df = spark.sql(query)\n",
    "\n",
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "df = spark.read.load('./data/sherlock_sentences.parquet')\n",
    "\n",
    "# Filter and show the first 5 rows\n",
    "df.where('id > 70').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "\n",
    "# Split the clause column into a column called words \n",
    "split_df = clauses_df.select(split('clause', ' ').alias('words'))\n",
    "split_df.show(5, truncate=False)\n",
    "\n",
    "# Explode the words column into a column called word \n",
    "exploded_df = split_df.select(explode('words').alias('word'))\n",
    "exploded_df.show(10)\n",
    "\n",
    "# Count the resulting number of rows in exploded_df\n",
    "print(\"\\nNumber of rows: \", exploded_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word for each row, previous two and subsequent two words\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT\n",
    "                        part,\n",
    "                        LAG(word, 2) OVER(PARTITION BY part ORDER BY id ASC) AS w1,\n",
    "                        LAG(word, 1) OVER(PARTITION BY part ORDER BY id ASC) AS w2,\n",
    "                        word AS w3,\n",
    "                        LEAD(word, 1) OVER(PARTITION BY part ORDER BY id ASC) AS w4,\n",
    "                        LEAD(word, 2) OVER(PARTITION BY part ORDER BY id ASC) AS w5\n",
    "                FROM text\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql_query).where(\"part = 12\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine that there are 12 chapters by the following:\n",
    "\n",
    "(text_df.select('chapter')\n",
    "       .distinct()\n",
    "       .sort('chapter')\n",
    "       .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the text_df into 12 partitions, with each chapter in its own partition.\n",
    "repart_df = text_df.repartition(12, 'chapter')\n",
    "\n",
    "# Display the number of partitions in the new dataframe.\n",
    "repart_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 sequences of five words\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT w1, w2, w3, w4, w5, COUNT(*) AS freq FROM (\n",
    "                   SELECT \n",
    "                       word AS w1,\n",
    "                       LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "                       LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "                       LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "                       LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "                   FROM text\n",
    "                )\n",
    "                GROUP BY w1, w2, w3, w4, w5\n",
    "                ORDER BY count DESC\n",
    "                LIMIT 10\n",
    "            \"\"\" \n",
    "\n",
    "\n",
    "df = spark.sql(sql_query)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique 5-tuples sorted alphabetically in descending order\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT DISTINCT w1, w2, w3, w4, w5 FROM (\n",
    "                   SELECT \n",
    "                       word AS w1,\n",
    "                       LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\n",
    "                       LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3,\n",
    "                       LEAD(word,3) OVER(PARTITION BY part ORDER BY id ) AS w4,\n",
    "                       LEAD(word,4) OVER(PARTITION BY part ORDER BY id ) AS w5\n",
    "                   FROM text\n",
    "                )\n",
    "                ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \n",
    "                LIMIT 10\n",
    "            \"\"\" \n",
    "\n",
    "\n",
    "df = spark.sql(sql_query)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Most frequent 3-tuple per chapter\n",
    "\n",
    "sql_query = \"\"\"\n",
    "                SELECT chapter, w1, w2, w3, count FROM\n",
    "                    (\n",
    "                      SELECT\n",
    "                      chapter,\n",
    "                      ROW_NUMBER() OVER (PARTITION BY chapter ORDER BY count DESC) AS row,\n",
    "                      w1, w2, w3, count\n",
    "                      FROM ( %s )\n",
    "                    )\n",
    "                WHERE row = 1\n",
    "                ORDER BY chapter ASC\n",
    "            \"\"\" % subquery\n",
    "\n",
    "spark.sql(sql_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A dataframe df1 is loaded from a csv file. Several processing steps are performed on it. As df1 is to \n",
    "be used more than once, it is a candidate for caching.\n",
    "\n",
    "A second dataframe df2 is created by performing additional compute-intensive steps on df1. It is also \n",
    "a candidate for caching.\n",
    "\n",
    "Because df2 depends on df1 the question arises: is it better to cache df1, or to cache df2?\n",
    "\"\"\"\n",
    "\n",
    "# Unpersists df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Cache df1\n",
    "df1.cache()\n",
    "\n",
    "\n",
    "# Run actions on both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "\n",
    "# Prove df1 is cached\n",
    "print(df1.is_cached)\n",
    "\n",
    "\n",
    "# Unpersist df1 and df2 and initializes a timer\n",
    "prep(df1, df2) \n",
    "\n",
    "# Persist df2 using memory and disk storage level \n",
    "df2.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Run actions both dataframes\n",
    "run(df1, \"df1_1st\") \n",
    "run(df1, \"df1_2nd\")\n",
    "run(df2, \"df2_1st\")\n",
    "run(df2, \"df2_2nd\", elapsed=True)\n",
    "\n",
    "\n",
    "# Cache df1, because it improves the time of the 2nd, 3rd, and 4th action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Caching and uncaching tables:\n",
    "\n",
    "A dataframe is cached using a cache() or persist() operation, a table is cached using a cacheTable() operation.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# List the tables\n",
    "print(\"Tables:\\n\", spark.catalog.listTables())\n",
    "\n",
    "# Cache table1 and Confirm that it is cached\n",
    "spark.catalog.cacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))\n",
    "\n",
    "# Uncache table1 and confirm that it is uncached\n",
    "spark.catalog.uncacheTable('table1')\n",
    "print(\"table1 is cached: \", spark.catalog.isCached('table1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark UI is available on http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable('schedule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log columns of text_df as debug message\n",
    "logging.debug(f\"df columns: {df.columns}\")\n",
    "\n",
    "# Log whether table1 is cached as info message\n",
    "logging.info(f'schedule is cached: {spark.catalog.isCached(tableName=\"schedule\")}')\n",
    "\n",
    "# Log first row of text_df as warning message\n",
    "logging.warning(f'The first row of text_df:\\n {df.first()}')\n",
    "\n",
    "# Log selected columns of text_df as error message\n",
    "logging.error(f'Selected columns: {df.select(\"id\", \"word\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the 5 statements that do NOT trigger df\n",
    "\n",
    "logging.debug(\"df columns: %s\", df.columns)\n",
    "\n",
    "logging.info(\"schedule is cached: %s\", spark.catalog.isCached(tableName=\"schedule\"))\n",
    "\n",
    "# logging.warning(\"The first row of df: %s\", df.first())\n",
    "\n",
    "logging.error(\"Selected columns: %s\", df.select(\"id\", \"word\"))\n",
    "\n",
    "logging.info(\"Tables: %s\", spark.sql(\"SHOW tables\").collect())\n",
    "\n",
    "logging.debug(\"First row: %s\", spark.sql(\"SELECT * FROM schedule LIMIT 1\"))\n",
    "\n",
    "# logging.debug(\"Count: %s\", spark.sql(\"SELECT COUNT(*) AS count FROM schedule\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR, format='%(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run explain on text_df\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run explain on \"SELECT COUNT(*) AS count FROM table1\" \n",
    "spark.sql(\"SELECT COUNT(*) AS count FROM schedule\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run explain on \"SELECT COUNT(DISTINCT word) AS words FROM table1\"\n",
    "spark.sql(\"SELECT COUNT(DISTINCT train_id) AS trains FROM schedule\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes your data needs a transformation that is not supported by built-in functions. \n",
    "# This is where a custom user defined function (\"UDF\") is suitable.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, StringType, ArrayType, FloatType,\n",
    "\n",
    "# Returns true if the value is a nonempty vector\n",
    "nonempty_udf = udf(lambda x:  \n",
    "    True if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "    else False, BooleanType())\n",
    "\n",
    "# Returns first element of the array as string\n",
    "s_udf = udf(lambda x: str(x[0]) if (x and type(x) is list and len(x) > 0) else '', StringType())\n",
    "\n",
    "# Returns true if the value is a nonempty vector\n",
    "short_udf = udf(lambda x: True if not x or len(x) < 10 else False, BooleanType())\n",
    "df.select(short_udf('textdata').alias(\"is short\")).show(3)\n",
    "\n",
    "# UDF removes items in TRIVIAL_TOKENS from array\n",
    "rm_trivial_udf = udf(lambda x:\n",
    "                     list(set(x) - TRIVIAL_TOKENS) if x\n",
    "                     else x,\n",
    "                     ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# Removes last item in array\n",
    "in_udf = udf(lambda x: x[0:len(x)-1] if x and len(x) > 1 else [],\n",
    "                ArrayType(StringType())\n",
    "                )\n",
    "\n",
    "# Returns first element of the array as Integer\n",
    "first_udf = udf(\n",
    "                lambda x: int(x.indices[0])\n",
    "                    if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "                    else 0,\n",
    "                IntegerType())\n",
    "\n",
    "# Selects the first element of a vector column\n",
    "first_udf = udf(lambda x:\n",
    "            float(x.indices[0]) \n",
    "            if (x and hasattr(x, \"toArray\") and x.numNonzeros())\n",
    "            else 0.0,\n",
    "            FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
